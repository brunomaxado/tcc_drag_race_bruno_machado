{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e558a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "df= pd.read_excel('queen_por_pontuacao.xlsx', sheet_name=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, drop_cols=[\"colocacao\", \"queen\"]):\n",
    "    # 1. ordenar por queen (id) e episódio\n",
    "    df = df.sort_values([\"id\", \"ep\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2. opcional: remover coluna de colocação para não vazar\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    # 3. codificar season (tempfranquia) se necessário - usar LabelEncoder ou one-hot\n",
    "    if \"tempfranquia\" in df.columns:\n",
    "        le_temp = LabelEncoder()\n",
    "        df[\"tempfranquia_le\"] = le_temp.fit_transform(df[\"tempfranquia\"].astype(str))\n",
    "    else:\n",
    "        df[\"tempfranquia_le\"] = 0\n",
    "\n",
    "    # 4. agrupar por queen (id) e montar sequências\n",
    "    groups = []\n",
    "    for gid, g in df.groupby(\"id\"):\n",
    "        g = g.sort_values(\"ep\")\n",
    "        # features por timestep: bom, ruim, media, idade, tempfranquia_le (idade e tempfranquia constantes por queen)\n",
    "        feat_cols = [c for c in [\"bom\", \"ruim\", \"media\", \"idade\", \"tempfranquia_le\"] if c in g.columns]\n",
    "        seq = g[feat_cols].values.astype(np.float32)\n",
    "        label = int(g[\"vencedora\"].iloc[0])  # label por queen (0/1)\n",
    "        queen_name = g[\"queen\"].iloc[0] if \"queen\" in g.columns else str(gid)\n",
    "        groups.append({\n",
    "            \"id\": gid,\n",
    "            \"queen\": queen_name,\n",
    "            \"seq\": seq,\n",
    "            \"length\": seq.shape[0],\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da403a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueenSequenceDataset(Dataset):\n",
    "    def __init__(self, groups, scaler=None):\n",
    "        self.groups = groups\n",
    "        # achatar todos os timesteps para treinar scaler se fornecido\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            all_timesteps = np.vstack([g[\"seq\"] for g in groups])\n",
    "            self.scaler.fit(all_timesteps)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        # aplicar scaler\n",
    "        for g in self.groups:\n",
    "            g[\"seq\"] = self.scaler.transform(g[\"seq\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g = self.groups[idx]\n",
    "        return torch.tensor(g[\"seq\"], dtype=torch.float32), g[\"length\"], torch.tensor(g[\"label\"], dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list of (seq_tensor, length, label)\n",
    "    seqs, lengths, labels = zip(*batch)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    # pad sequences\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True)  # (B, T_max, D)\n",
    "    return seqs_padded, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5258508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, bidirectional=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0)\n",
    "        mult = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * mult, 1)\n",
    "\n",
    "    def forward(self, x, lengths, return_seq=False):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, h_n = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, H*mult)\n",
    "\n",
    "        if return_seq:\n",
    "            # saída em cada timestep\n",
    "            logits = self.fc(out).squeeze(-1)  # (B, T)\n",
    "            return logits\n",
    "        else:\n",
    "            # só último estado\n",
    "            if self.bidirectional:\n",
    "                last_fw = h_n[-2, :, :]\n",
    "                last_bw = h_n[-1, :, :]\n",
    "                h = torch.cat([last_fw, last_bw], dim=1)\n",
    "            else:\n",
    "                h = h_n[-1, :, :]\n",
    "            logits = self.fc(h).squeeze(1)\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199688b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_episode_table(results):\n",
    "    # descobrir o máximo de episódios\n",
    "    max_len = max(len(r[\"probs\"]) for r in results)\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        row = [r[\"queen\"]]\n",
    "        row += [f\"{p:.2f}\" for p in r[\"probs\"]]\n",
    "        # padding se tiver menos episódios\n",
    "        while len(row) < max_len + 1:\n",
    "            row.append(\"\")\n",
    "        rows.append(row)\n",
    "    cols = [\"queen\"] + [f\"ep{i+1}\" for i in range(max_len)]\n",
    "    return pd.DataFrame(rows, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7a5908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_episode_by_episode(model, dataset, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            seq, length, label = dataset[i]\n",
    "            seq = seq.unsqueeze(0).to(device)   # (1, T, D)\n",
    "            length = torch.tensor([length])\n",
    "            logits = model(seq, length, return_seq=True)  # (1, T)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "            queen_id = dataset.groups[i][\"id\"]\n",
    "            all_results.append({\n",
    "                \"id\": queen_id,\n",
    "                \"queen\": dataset.groups[i].get(\"queen\", str(queen_id)),\n",
    "                \"probs\": probs,\n",
    "                \"label\": int(label.item())\n",
    "            })\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770f72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, opt, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for seqs, lengths, labels in loader:\n",
    "        seqs, lengths, labels = seqs.to(device), lengths.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(seqs, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * seqs.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    probs = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths, labels in loader:\n",
    "            seqs, lengths = seqs.to(device), lengths.to(device)\n",
    "            logits = model(seqs, lengths)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            pred = (prob >= 0.5).astype(int)\n",
    "            preds.extend(pred.tolist())\n",
    "            probs.extend(prob.tolist())\n",
    "            trues.extend(labels.numpy().astype(int).tolist())\n",
    "    # métricas\n",
    "    p, r, f1, _ = precision_recall_fscore_support(trues, preds, average=\"binary\", zero_division=0)\n",
    "    auc = None\n",
    "    try:\n",
    "        auc = roc_auc_score(trues, probs)\n",
    "    except:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1, \"auc\": auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c61d3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(df,\n",
    "                 test_size=0.2,\n",
    "                 random_state=42,\n",
    "                 batch_size=16,\n",
    "                 hidden_size=64,\n",
    "                 epochs=30,\n",
    "                 device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    groups = prepare_sequences(df)\n",
    "\n",
    "    # dividir em treino/val por tempfranquia (evita vazar informações entre temporadas)\n",
    "    # criar lista de season por id (assumindo season é igual nas linhas de um id)\n",
    "    # se não houver tempfranquia, faremos split randômico por id\n",
    "    df_id2season = df.groupby(\"id\")[\"tempfranquia\"].first() if \"tempfranquia\" in df.columns else None\n",
    "    ids = [g[\"id\"] for g in groups]\n",
    "    if df_id2season is not None:\n",
    "        id_list = np.array(ids)\n",
    "        seasons = np.array([str(df_id2season[i]) for i in id_list])\n",
    "        # stratify by season could still leak winners distribution; uma alternativa simples: stratify por label\n",
    "        labels = np.array([g[\"label\"] for g in groups])\n",
    "        train_idx, test_idx = train_test_split(range(len(groups)), test_size=test_size, random_state=random_state, stratify=labels)\n",
    "    else:\n",
    "        labels = np.array([g[\"label\"] for g in groups])\n",
    "        train_idx, test_idx = train_test_split(range(len(groups)), test_size=test_size, random_state=random_state, stratify=labels)\n",
    "\n",
    "    train_groups = [groups[i] for i in train_idx]\n",
    "    test_groups = [groups[i] for i in test_idx]\n",
    "\n",
    "    # criar scaler a partir de treino\n",
    "    dataset_train = QueenSequenceDataset(train_groups, scaler=None)\n",
    "    scaler = dataset_train.scaler\n",
    "    dataset_test = QueenSequenceDataset(test_groups, scaler=scaler)\n",
    "\n",
    "    # lidar com imbalance: WeightedRandomSampler por label na dataset_train\n",
    "    labels_train = np.array([g[\"label\"] for g in train_groups])\n",
    "    class_counts = Counter(labels_train.tolist())\n",
    "    # se classe 1 muito rara, dar weight inverso da frequência\n",
    "    class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "    sample_weights = np.array([class_weights[int(lbl)] for lbl in labels_train])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    input_size = dataset_train[0][0].shape[1]\n",
    "    model = GRUClassifier(input_size=input_size, hidden_size=hidden_size, bidirectional=True).to(device)\n",
    "\n",
    "    # Loss com peso de classes (opcional)\n",
    "    # calcular peso para classe positiva na BCEWithLogitsLoss\n",
    "    pos_weight = None\n",
    "    if 1 in class_counts:\n",
    "        neg = class_counts.get(0, 0)\n",
    "        pos = class_counts.get(1, 0)\n",
    "        if pos == 0:\n",
    "            pos_weight = None\n",
    "        else:\n",
    "            pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    if pos_weight is not None:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(model, loader_train, opt, criterion, device)\n",
    "        metrics = eval_model(model, loader_test, device)\n",
    "        print(f\"Epoch {epoch:02d} | Loss {loss:.4f} | val f1 {metrics['f1']:.4f} prec {metrics['precision']:.4f} rec {metrics['recall']:.4f} auc {metrics['auc']:.4f}\")\n",
    "        if metrics[\"f1\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1\"]\n",
    "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    # carregar melhor modelo\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    final_metrics = eval_model(model, loader_test, device)\n",
    "    print(\"== Final metrics on test set:\", final_metrics)\n",
    "    return {\"model\": model, \"scaler\": scaler, \"dataset_train\": dataset_train, \"dataset_test\": dataset_test, \"metrics\": final_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4391003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline2(df,\n",
    "                 test_size=0.2,\n",
    "                 random_state=42,\n",
    "                 batch_size=16,\n",
    "                 hidden_size=64,\n",
    "                 epochs=30,\n",
    "                 device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    groups = prepare_sequences(df)\n",
    "\n",
    "    # dividir em treino/val por tempfranquia (evita vazar informações entre temporadas)\n",
    "    # criar lista de season por id (assumindo season é igual nas linhas de um id)\n",
    "    # se não houver tempfranquia, faremos split randômico por id\n",
    "    # ----------------------------\n",
    "# Divisão treino/teste por tempfranquia\n",
    "# ----------------------------\n",
    "    season_to_ids = df.groupby(\"tempfranquia\")[\"id\"].unique().to_dict()\n",
    "\n",
    "    import random\n",
    "    random.seed(random_state)\n",
    "\n",
    "    seasons = list(season_to_ids.keys())\n",
    "    random.shuffle(seasons)\n",
    "\n",
    "    n_test = max(1, int(len(seasons) * test_size))\n",
    "    test_seasons = seasons[:n_test]\n",
    "    train_seasons = seasons[n_test:]\n",
    "\n",
    "    train_ids = [i for s in train_seasons for i in season_to_ids[s]]\n",
    "    test_ids  = [i for s in test_seasons for i in season_to_ids[s]]\n",
    "\n",
    "    train_groups = [g for g in groups if g[\"id\"] in train_ids]\n",
    "    test_groups  = [g for g in groups if g[\"id\"] in test_ids]\n",
    "\n",
    "\n",
    "    # criar scaler a partir de treino\n",
    "    dataset_train = QueenSequenceDataset(train_groups, scaler=None)\n",
    "    scaler = dataset_train.scaler\n",
    "    dataset_test = QueenSequenceDataset(test_groups, scaler=scaler)\n",
    "\n",
    "    # lidar com imbalance: WeightedRandomSampler por label na dataset_train\n",
    "    labels_train = np.array([g[\"label\"] for g in train_groups])\n",
    "    class_counts = Counter(labels_train.tolist())\n",
    "    # se classe 1 muito rara, dar weight inverso da frequência\n",
    "    class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "    sample_weights = np.array([class_weights[int(lbl)] for lbl in labels_train])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    input_size = dataset_train[0][0].shape[1]\n",
    "    model = GRUClassifier(input_size=input_size, hidden_size=hidden_size, bidirectional=True).to(device)\n",
    "\n",
    "    # Loss com peso de classes (opcional)\n",
    "    # calcular peso para classe positiva na BCEWithLogitsLoss\n",
    "    pos_weight = None\n",
    "    if 1 in class_counts:\n",
    "        neg = class_counts.get(0, 0)\n",
    "        pos = class_counts.get(1, 0)\n",
    "        if pos == 0:\n",
    "            pos_weight = None\n",
    "        else:\n",
    "            pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(device)\n",
    "\n",
    "    if pos_weight is not None:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(model, loader_train, opt, criterion, device)\n",
    "        metrics = eval_model(model, loader_test, device)\n",
    "        print(f\"Epoch {epoch:02d} | Loss {loss:.4f} | val f1 {metrics['f1']:.4f} prec {metrics['precision']:.4f} rec {metrics['recall']:.4f} auc {metrics['auc']:.4f}\")\n",
    "        if metrics[\"f1\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1\"]\n",
    "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    # carregar melhor modelo\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    final_metrics = eval_model(model, loader_test, device)\n",
    "    print(\"== Final metrics on test set:\", final_metrics)\n",
    "    return {\"model\": model, \"scaler\": scaler, \"dataset_train\": dataset_train, \"dataset_test\": dataset_test, \"metrics\": final_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35430100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss 1.4179 | val f1 0.3235 prec 0.1930 rec 1.0000 auc 0.9009\n",
      "Epoch 02 | Loss 0.8893 | val f1 0.3548 prec 0.2157 rec 1.0000 auc 0.9034\n",
      "Epoch 03 | Loss 0.8510 | val f1 0.3284 prec 0.1964 rec 1.0000 auc 0.9132\n",
      "Epoch 04 | Loss 0.7883 | val f1 0.3860 prec 0.2391 rec 1.0000 auc 0.9124\n",
      "Epoch 05 | Loss 0.7221 | val f1 0.4231 prec 0.2683 rec 1.0000 auc 0.9238\n",
      "Epoch 06 | Loss 0.6104 | val f1 0.4314 prec 0.2750 rec 1.0000 auc 0.9378\n",
      "Epoch 07 | Loss 0.5714 | val f1 0.4400 prec 0.2821 rec 1.0000 auc 0.9451\n",
      "Epoch 08 | Loss 0.4920 | val f1 0.4583 prec 0.2973 rec 1.0000 auc 0.9541\n",
      "Epoch 09 | Loss 0.6120 | val f1 0.4314 prec 0.2750 rec 1.0000 auc 0.9451\n",
      "Epoch 10 | Loss 0.4091 | val f1 0.4651 prec 0.3125 rec 0.9091 auc 0.9517\n",
      "== Final metrics on test set: {'precision': 0.3125, 'recall': 0.9090909090909091, 'f1': 0.4651162790697674, 'auc': 0.9516789516789517}\n"
     ]
    }
   ],
   "source": [
    "cols = [\"id\", \"queen\", \"ep\", \"bom\", \"ruim\", \"media\", \"colocacao\", \"idade\", \"tempfranquia\", \"vencedora\"]\n",
    "out = run_pipeline2(df, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a35ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen  ep1  ep2  ep3  ep4  ep5  ep6  ep7  ep8  ep9 ep10 ep11 ep12 ep13\n",
      "  384 0.90 0.94 0.93 0.98 0.98 0.99 0.90                              \n",
      "  115 0.22 0.06                                                       \n",
      "  206 0.91 0.91 0.91 0.92 0.96 0.94 0.94 0.96 0.94 0.73 0.39          \n",
      "  660 0.02 0.02 0.04 0.02                                             \n",
      "   25 0.84 0.90 0.94 0.92 0.82 0.96 0.95 0.70 0.97 0.96 0.90 0.43     \n",
      "  233 0.78 0.87 0.86 0.59                                             \n",
      "  181 0.01 0.01 0.00 0.01                                             \n",
      "  360 0.02 0.02 0.02                                                  \n",
      "  434 0.11                                                            \n",
      "   96 0.02 0.02 0.00 0.01 0.01 0.01                                   \n",
      "  556 0.09 0.08 0.04                                                  \n",
      "  510 0.30 0.08                                                       \n",
      "  439 0.04 0.06 0.02 0.01 0.04 0.01 0.00 0.00 0.01                    \n",
      "   31 0.06 0.07 0.03 0.04                                             \n",
      "  308 0.04 0.06 0.02 0.06 0.03 0.03 0.08 0.05 0.04                    \n",
      "  658 0.89 0.95 0.81 0.95 0.62                                        \n",
      "   71 0.18 0.27 0.37 0.10                                             \n",
      "  538 0.06 0.17 0.18 0.15 0.13 0.10 0.08 0.05                         \n",
      "  616 0.01 0.03 0.01 0.00 0.01                                        \n",
      "  375 0.08 0.05 0.09 0.03 0.18 0.33 0.17                              \n",
      "  536 0.07 0.13 0.13 0.04 0.08 0.53 0.31 0.24                         \n",
      "  467 0.61 0.16                                                       \n",
      "  147 0.94 0.95 0.98 0.99 0.99 1.00 0.99 0.98 0.98 0.97 0.98          \n",
      "  618 0.32 0.38 0.10                                                  \n",
      "  204 0.93 0.96 0.98 0.88 0.99 0.98 0.98 0.99 0.96 0.99 0.92 0.98 0.58\n",
      "  170 0.33 0.09                                                       \n",
      "  336 0.06 0.05 0.02 0.19 0.04 0.14 0.13                              \n",
      "  570 0.01 0.01 0.01 0.01 0.00 0.00 0.01 0.01                         \n",
      "  519 0.02 0.04 0.02                                                  \n",
      "  498 0.31 0.37 0.10                                                  \n",
      "  108 0.82 0.92 0.94 0.96 0.95 0.94 0.95 0.90 0.48                    \n",
      "  521 0.10                                                            \n",
      "  236 0.29 0.08                                                       \n",
      "  504 0.89 0.88 0.88 0.95 0.63 0.95 0.88 0.48                         \n",
      "  301 0.07 0.04 0.16 0.09 0.23 0.13                                   \n",
      "  294 0.10                                                            \n",
      "  452 0.06 0.05 0.01 0.10 0.15 0.05 0.25 0.10 0.11                    \n",
      "  424 0.84 0.83 0.92 0.96 0.93 0.70 0.97 0.87 0.56                    \n",
      "  218 0.02 0.01 0.01 0.00 0.00 0.00 0.00                              \n",
      "  582 0.92 0.95 0.94 0.47                                             \n",
      "  344 0.80 0.86 0.91 0.94 0.80 0.89 0.89 0.93                         \n",
      "  377 0.01 0.01 0.00 0.00 0.00 0.00 0.00                              \n",
      "  516 0.05 0.02 0.06 0.12 0.31 0.12 0.16                              \n",
      "   37 0.75 0.69 0.67 0.80 0.48 0.89 0.90 0.46 0.71 0.88 0.85          \n",
      "  400 0.09 0.07 0.04                                                  \n",
      "  140 0.07 0.05 0.29 0.36 0.12                                        \n",
      "  629 0.35 0.43 0.11                                                  \n",
      "  112 0.54 0.53 0.53 0.56                                             \n",
      "  142 0.23 0.32 0.08                                                  \n",
      "  577 0.05 0.06 0.04 0.22 0.13 0.25 0.13                              \n",
      "  117 0.84 0.54 0.75 0.70 0.66 0.60 0.51 0.43 0.30 0.22 0.14 0.08     \n",
      "  503 0.04 0.02 0.01 0.02 0.01 0.02 0.02 0.02                         \n",
      "   65 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     \n",
      "  341 0.09 0.03                                                       \n",
      "  251 0.07 0.07 0.02 0.07 0.05 0.02 0.05 0.01 0.00 0.00               \n",
      "  393 0.83 0.93 0.95 0.97 0.98 0.98 0.98                              \n",
      "  565 0.08 0.04                                                       \n",
      "   53 0.04 0.03 0.03 0.11 0.07 0.04 0.03 0.08 0.05                    \n",
      "  426 0.06 0.07 0.08 0.21 0.24 0.22 0.24 0.89 0.44                    \n",
      "  444 0.03 0.09 0.15 0.41 0.09                                        \n",
      "  293 0.58 0.15                                                       \n",
      "  368 0.07 0.21 0.30 0.14 0.15                                        \n",
      "  421 0.39 0.10                                                       \n",
      "   17 0.14 0.23 0.34 0.16 0.19                                        \n",
      "  322 0.04 0.10 0.05 0.11 0.08 0.06 0.04 0.04                         \n",
      "  625 0.04 0.03 0.07 0.06 0.02 0.06 0.06                              \n",
      "   69 0.04 0.04 0.04 0.01 0.08 0.23 0.07                              \n",
      "  655 0.93 0.96 0.97 0.97 0.97                                        \n",
      "  194 0.06 0.03 0.05 0.02 0.02 0.01 0.01                              \n",
      "  370 0.23 0.42 0.10                                                  \n",
      "  429 0.04 0.08 0.03 0.11 0.06 0.08                                   \n",
      "  567 0.91 0.95 0.98 0.99 0.99 0.99 0.96 0.98                         \n",
      "   16 0.01 0.03 0.03 0.04 0.02 0.02                                   \n",
      "  224 0.13                                                            \n",
      "  601 0.04 0.01 0.02 0.00 0.00 0.00 0.00 0.00 0.00                    \n",
      "   39 0.02 0.02 0.03 0.01 0.01 0.01                                   \n",
      "  539 0.91 0.91 0.95 0.97 0.95 0.98 0.98 0.74                         \n",
      "  262 0.88 0.87 0.93 0.97 0.98 0.97 0.85 0.98 0.94                    \n",
      "  573 0.02 0.04 0.09 0.04                                             \n",
      "  176 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.01 0.01 0.01 0.01          \n",
      "  591 0.05 0.14 0.26 0.10 0.18 0.21                                   \n",
      "  545 0.10                                                            \n",
      "  670 0.91 0.96 0.87 0.92 0.93 0.92 0.37                              \n",
      "    6 0.10 0.18 0.17 0.11                                             \n",
      "  550 0.08 0.08 0.09 0.10 0.03 0.59 0.62 0.28 0.23                    \n",
      "  314 0.02 0.01 0.02 0.01 0.01                                        \n",
      "  620 0.94 0.95 0.95 0.98 0.97 0.97 0.99 0.98 0.98                    \n",
      "  100 0.21 0.06                                                       \n",
      "  610 0.86 0.87 0.97 0.95 0.98 0.96 0.98 0.98 0.99 0.86               \n",
      "  298 0.91 0.95 0.94 0.86 0.98 0.96 0.83 0.89 0.32                    \n",
      "  415 0.92 0.93 0.97 0.97 0.95 0.69 0.55                              \n",
      "  512 0.59 0.65 0.58 0.78 0.29 0.78 0.66                              \n",
      "  391 0.28 0.08                                                       \n",
      "  530 0.64 0.26 0.77 0.19                                             \n",
      "  502 0.93 0.96 0.98 0.94 0.96 0.75 0.98 0.96                         \n",
      "  151 0.92 0.91 0.87 0.89 0.78 0.76 0.34 0.34                         \n",
      "  223 0.39 0.11                                                       \n",
      "  277 0.06 0.03 0.04 0.04 0.02 0.03 0.02                              \n",
      "   45 0.13 0.07 0.05                                                  \n",
      "  383 0.52 0.77 0.87 0.93 0.97 0.96 0.96                              \n",
      "  263 0.83 0.89 0.95 0.98 0.97 0.98 0.96 0.94 0.70                    \n",
      "  133 0.86 0.86 0.94 0.97 0.90 0.98 0.96 0.99 0.98 0.92 0.73 0.80     \n",
      "  614 0.01 0.00 0.00 0.00 0.00 0.00 0.00                              \n",
      "  428 0.52 0.61 0.54 0.56 0.34 0.90 0.22                              \n",
      "  457 0.07 0.06 0.04                                                  \n",
      "  476 0.29 0.08                                                       \n",
      "  522 0.86 0.93 0.92 0.98 0.98 0.97 0.92 0.97 0.94                    \n",
      "  275 0.06 0.12 0.11 0.03 0.25 0.12 0.12 0.76 0.16                    \n",
      "   58 0.07 0.17 0.13 0.07                                             \n",
      "   24 0.04 0.10 0.23 0.21 0.45 0.17 0.34 0.84 0.42 0.49 0.76 0.40     \n",
      "  175 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01     \n",
      "  407 0.07 0.07 0.02 0.18 0.38 0.13 0.19                              \n",
      "  274 0.07 0.07 0.13 0.32 0.35 0.20 0.75 0.20 0.56                    \n",
      "  497 0.24 0.29 0.57 0.17                                             \n",
      "  219 0.05 0.02 0.06 0.08 0.10 0.05                                   \n",
      "   62 0.93 0.97 0.98 0.98 0.99 0.99 0.99 1.00 0.99 1.00 0.99 0.99     \n",
      "  265 0.71 0.64 0.62 0.63 0.79 0.81 0.90 0.33                         \n",
      "   88 0.09 0.03                                                       \n",
      "   79 0.86 0.83 0.50 0.94 0.91 0.96 0.97 0.94 0.96 0.91 0.64 0.42     \n",
      "  673 0.72 0.64 0.60 0.15                                             \n",
      "    3 0.66 0.75 0.79 0.36 0.77 0.39                                   \n",
      "   46 0.44 0.13                                                       \n",
      "  559 0.08 0.07 0.49 0.19 0.56 0.33 0.18                              \n",
      "  593 0.13 0.23 0.35 0.09                                             \n",
      "  235 0.70 0.67 0.18                                                  \n",
      "  307 0.91 0.69 0.84 0.82 0.93 0.96 0.94 0.59 0.95                    \n",
      "  520 0.46 0.13                                                       \n",
      "  505 0.21 0.51 0.51 0.80 0.23 0.30                                   \n",
      "  552 0.04 0.04 0.08 0.08 0.15 0.08 0.18                              \n",
      "  546 0.91 0.91 0.97 0.98 0.98 0.98 0.99 0.99 0.98                    \n",
      "  109 0.04 0.03 0.01 0.05 0.01 0.01 0.01 0.01                         \n",
      "  589 0.05 0.06 0.06 0.08 0.04 0.38 0.08 0.76 0.25                    \n",
      "  414 0.85 0.84 0.83 0.93 0.43 0.91 0.46                              \n",
      "  448 0.83 0.89 0.93 0.92 0.97 0.95 0.98 0.98 0.90 0.97               \n",
      "  208 0.91 0.91 0.97 0.95 0.86 0.97 0.89 0.43                         \n"
     ]
    }
   ],
   "source": [
    "results = predict_episode_by_episode(out[\"model\"], out[\"dataset_test\"], device=\"cpu\")\n",
    "df_table = make_episode_table(results)\n",
    "print(df_table.to_string(index=False))\n",
    "\n",
    "df_table.to_excel(\"AAAAAAAA2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c90b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_queen= pd.read_excel('TCC DADOS3.xlsx', sheet_name=0)\n",
    "dim_queen = dim_queen.rename(columns={\"ID\": \"queen\" , \"queen\":\"nome\", \"temp\":\"temp\", \"franquia \" : \"franquia\"})\n",
    "dim_queen['queen'] = dim_queen['queen'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "849f0821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queen</th>\n",
       "      <th>nome</th>\n",
       "      <th>idade</th>\n",
       "      <th>colocacao</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>temp</th>\n",
       "      <th>franquia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nina Flowers</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rebecca Glasscock</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Flórida</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Shannel</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ongina</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Califórnia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>671</td>\n",
       "      <td>Gala Varo</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>Morelia</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>672</td>\n",
       "      <td>Soa de Muse</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>Saint-Denis</td>\n",
       "      <td>France</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>673</td>\n",
       "      <td>Eva Le Queen</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>Marikina</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>674</td>\n",
       "      <td>Miranda Lebrão</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>675</td>\n",
       "      <td>Athena Likis</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>City of Brussels</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    queen               nome  idade  colocacao            cidade    estado     \\\n",
       "0       1  BeBe Zahara Benet     28          1       Minneapolis    Minnesota   \n",
       "1       2       Nina Flowers     34          2            Denver     Colorado   \n",
       "2       3  Rebecca Glasscock     26          3   Fort Lauderdale      Flórida   \n",
       "3       4            Shannel     29          4         Las Vegas       Nevada   \n",
       "4       5             Ongina     26          5       Los Angeles   Califórnia   \n",
       "..    ...                ...    ...        ...               ...          ...   \n",
       "670   671          Gala Varo     34          8           Morelia       Mexico   \n",
       "671   672        Soa de Muse     34          9       Saint-Denis       France   \n",
       "672   673       Eva Le Queen     35         10          Marikina  Philippines   \n",
       "673   674     Miranda Lebrão     34         11    Rio de Janeiro       Brazil   \n",
       "674   675       Athena Likis     27         12  City of Brussels      Belgium   \n",
       "\n",
       "     temp   franquia  \n",
       "0        1         1  \n",
       "1        1         1  \n",
       "2        1         1  \n",
       "3        1         1  \n",
       "4        1         1  \n",
       "..     ...       ...  \n",
       "670     19         1  \n",
       "671     19         1  \n",
       "672     19         1  \n",
       "673     19         1  \n",
       "674     19         1  \n",
       "\n",
       "[675 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f494ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen  ep1  ep2  ep3  ep4  ep5  ep6  ep7  ep8  ep9 ep10 ep11 ep12 ep13\n",
      "   35 0.95 0.96 0.98 0.97 0.99 0.99 1.00 0.98 0.99 0.97 0.99          \n",
      "   36 0.86 0.90 0.92 0.96 0.99 0.99 0.99 0.99 0.99 0.98 0.90          \n",
      "   37 0.03 0.01 0.01 0.01 0.00 0.02 0.08 0.04 0.09 0.71 0.92          \n",
      "   38 0.02 0.01 0.00 0.01 0.01 0.10 0.07 0.80 0.25 0.10 0.10          \n",
      "   39 0.01 0.01 0.01 0.00 0.00 0.00                                   \n",
      "   40 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00                    \n",
      "   41 0.72 0.45 0.26 0.27 0.29 0.49 0.14                              \n",
      "   42 0.01 0.00 0.00 0.00 0.01 0.00 0.01                              \n",
      "   43 0.03 0.02 0.01 0.00 0.00 0.01                                   \n",
      "   44 0.34 0.32 0.08 0.01                                             \n",
      "   45 0.30 0.04 0.01                                                  \n",
      "   46 0.47 0.05                                                       \n",
      "   47 0.06                                                            \n",
      "  249 0.83 0.88 0.82 0.93 0.97 0.97 0.99 0.99 0.99                    \n",
      "  250 0.83 0.86 0.56 0.85 0.83 0.99 0.99 0.99 0.97                    \n",
      "  251 0.05 0.02 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01               \n",
      "  252 0.02 0.00 0.00 0.00 0.00 0.01 0.02 0.01 0.10 0.05               \n",
      "  253 0.03 0.04 0.03 0.05 0.09 0.05 0.05 0.40 0.10                    \n",
      "  254 0.01 0.02 0.04 0.05 0.02 0.04 0.02                              \n",
      "  255 0.11 0.15 0.18 0.06 0.08 0.04                                   \n",
      "  256 0.19 0.08 0.12 0.02 0.01                                        \n",
      "  257 0.45 0.11 0.13 0.02                                             \n",
      "  258 0.13 0.20 0.02                                                  \n",
      "  259 0.47 0.05                                                       \n",
      "  260 0.06                                                            \n",
      "  261 0.91 0.81 0.72 0.90 0.96 0.99 0.99 1.00 0.99                    \n",
      "  262 0.92 0.84 0.87 0.94 0.96 0.96 0.87 0.99 0.99                    \n",
      "  263 0.82 0.80 0.85 0.91 0.88 0.93 0.93 0.94 0.80                    \n",
      "  264 0.02 0.02 0.02 0.01 0.00 0.00 0.00 0.00 0.00                    \n",
      "  265 0.66 0.37 0.20 0.12 0.14 0.18 0.36 0.07                         \n",
      "  266 0.10 0.02 0.06 0.01 0.05 0.10 0.03                              \n",
      "  267 0.85 0.62 0.52 0.16 0.57 0.08                                   \n",
      "  268 0.33 0.31 0.18 0.06 0.01                                        \n",
      "  269 0.10 0.01 0.01 0.00                                             \n",
      "  270 0.47 0.05                                                       \n",
      "  319 0.70 0.39 0.20 0.32 0.49 0.24 0.23 0.92                         \n",
      "  320 0.05 0.00 0.02 0.00 0.06 0.58 0.64 0.83                         \n",
      "  321 0.73 0.70 0.63 0.60 0.17 0.39 0.65 0.28                         \n",
      "  322 0.05 0.05 0.01 0.01 0.02 0.08 0.25 0.67                         \n",
      "  323 0.70 0.42 0.23 0.15 0.19 0.26 0.54 0.10                         \n",
      "  324 0.03 0.00 0.01 0.00 0.02 0.01 0.03                              \n",
      "  325 0.06 0.02 0.01 0.02 0.01 0.01                                   \n",
      "  326 0.85 0.65 0.56 0.37 0.05                                        \n",
      "  327 0.43 0.20 0.04 0.01                                             \n",
      "  328 0.35 0.25 0.03                                                  \n",
      "  329 0.05 0.01                                                       \n",
      "  330 0.06                                                            \n",
      "  383 0.46 0.70 0.89 0.95 0.99 0.99 0.98                              \n",
      "  384 0.91 0.90 0.80 0.91 0.95 0.98 0.93                              \n",
      "  385 0.01 0.00 0.00 0.00 0.04 0.06 0.59                              \n",
      "  386 0.11 0.07 0.04 0.01 0.01 0.07 0.05                              \n",
      "  387 0.04 0.02 0.00 0.00 0.00 0.00                                   \n",
      "  388 0.70 0.22 0.36 0.39 0.05                                        \n",
      "  389 0.51 0.39 0.26 0.07                                             \n",
      "  390 0.34 0.13 0.01                                                  \n",
      "  391 0.31 0.02                                                       \n",
      "  435 0.80 0.63 0.78 0.88 0.85 0.95 0.78 0.98 0.98                    \n",
      "  436 0.06 0.01 0.02 0.03 0.31 0.89 0.98 0.58 0.89                    \n",
      "  437 0.89 0.76 0.59 0.48 0.65 0.77 0.81 0.89 0.53                    \n",
      "  438 0.01 0.00 0.00 0.00 0.00 0.01 0.03                              \n",
      "  439 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.04 0.04                    \n",
      "  440 0.93 0.90 0.85 0.71 0.42 0.65 0.67 0.19                         \n",
      "  441 0.07 0.01 0.02 0.04 0.01 0.03 0.03                              \n",
      "  442 0.83 0.85 0.67 0.56 0.84 0.82 0.47                              \n",
      "  443 0.31 0.22 0.10 0.03 0.18 0.03                                   \n",
      "  444 0.15 0.13 0.08 0.14 0.02                                        \n",
      "  445 0.39 0.27 0.02                                                  \n",
      "  446 0.06 0.01                                                       \n",
      "  447 0.07                                                            \n",
      "  460 0.64 0.36 0.21 0.17 0.40                                        \n",
      "  461 0.91 0.82 0.67 0.61 0.90                                        \n",
      "  462 0.29 0.03 0.02 0.37 0.95                                        \n",
      "  463 0.06 0.02 0.10 0.60 0.58                                        \n",
      "  464 0.07 0.03 0.01 0.02 0.07                                        \n",
      "  465 0.11 0.06 0.20 0.04                                             \n",
      "  466 0.70 0.74 0.64                                                  \n",
      "  467 0.60 0.06                                                       \n",
      "  501 0.92 0.91 0.92 0.95 0.98 0.99 1.00 0.99                         \n",
      "  502 0.95 0.95 0.96 0.85 0.92 0.59 0.98 0.98                         \n",
      "  503 0.08 0.01 0.01 0.01 0.01 0.04 0.20 0.23                         \n",
      "  504 0.85 0.64 0.38 0.59 0.12 0.77 0.88 0.63                         \n",
      "  505 0.45 0.42 0.22 0.40 0.05 0.04                                   \n",
      "  506 0.03 0.00 0.00 0.00 0.01 0.13 0.79 0.22                         \n",
      "  507 0.78 0.55 0.68 0.49 0.67 0.77 0.40                              \n",
      "  508 0.03 0.05 0.13 0.07 0.19                                        \n",
      "  509 0.05 0.03 0.01                                                  \n",
      "  510 0.32 0.02                                                       \n",
      "  511 0.07                                                            \n",
      "  512 0.60 0.43 0.21 0.29 0.05 0.18 0.25                              \n",
      "  513 0.05 0.01 0.00 0.02 0.19 0.86 0.97                              \n",
      "  514 0.91 0.89 0.88 0.92 0.88 0.94 0.76                              \n",
      "  515 0.49 0.31 0.48 0.54 0.63 0.73 0.90                              \n",
      "  516 0.08 0.02 0.02 0.04 0.08 0.02 0.01                              \n",
      "  517 0.47 0.43 0.13 0.19 0.15 0.02                                   \n",
      "  518 0.70 0.26 0.15 0.22 0.03                                        \n",
      "  519 0.04 0.03 0.01                                                  \n",
      "  520 0.49 0.05                                                       \n",
      "  521 0.06                                                            \n",
      "  631 0.83 0.82 0.91 0.95 0.96 0.99 1.00 0.99 0.99 1.00 1.00 0.98 0.99\n",
      "  632 0.12 0.06 0.04 0.03 0.03 0.09 0.22 0.64 0.33 0.42 0.67 0.26 0.87\n",
      "  633 0.93 0.87 0.79 0.78 0.84 0.97 0.98 0.99 0.94 0.98 0.98 0.92 0.97\n",
      "  634 0.74 0.68 0.50 0.22 0.55 0.75 0.95 0.94 0.97 0.97 0.83 0.97 0.80\n",
      "  635 0.94 0.89 0.91 0.97 0.97 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.89\n",
      "  636 0.02 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.05 0.05 0.02          \n",
      "  637 0.02 0.01 0.00 0.00 0.00 0.00 0.01 0.01 0.22 0.17               \n",
      "  638 0.03 0.02 0.00 0.00 0.03 0.04 0.38 0.86 0.07                    \n",
      "  639 0.02 0.00 0.00 0.00 0.00 0.01 0.02 0.03                         \n",
      "  640 0.01 0.00 0.00 0.01 0.01 0.05 0.07                              \n",
      "  641 0.88 0.72 0.67 0.48 0.64 0.10                                   \n",
      "  642 0.04 0.03 0.03 0.09 0.03                                        \n",
      "  643 0.23 0.02 0.01                                                  \n",
      "  644 0.17 0.01 0.00                                                  \n",
      "  645 0.82 0.58 0.31 0.21 0.20 0.10 0.10 0.92 0.99                    \n",
      "  646 0.02 0.00 0.01 0.00 0.02 0.01 0.30 0.36 0.06                    \n",
      "  647 0.24 0.13 0.02 0.10 0.22 0.37 0.80 0.36 0.76                    \n",
      "  648 0.01 0.00 0.00 0.01 0.01 0.14 0.67 0.13 0.22                    \n",
      "  649 0.51 0.48 0.51 0.76 0.76 0.94 0.80 0.93 0.32                    \n",
      "  650 0.26 0.26 0.25 0.08 0.03 0.48 0.09                              \n",
      "  651 0.06 0.05 0.01 0.01 0.21 0.12                                   \n",
      "  652 0.63 0.30 0.28 0.32 0.05                                        \n",
      "  653 0.11 0.06 0.01                                                  \n",
      "  654 0.51 0.05                                                       \n"
     ]
    }
   ],
   "source": [
    "results = predict_episode_by_episode(out[\"model\"], out[\"dataset_test\"], device=\"cpu\")\n",
    "df_table = make_episode_table(results)\n",
    "print(df_table.to_string(index=False))\n",
    "df_table = df_table.merge(\n",
    "    dim_queen[['queen', 'nome']],  # 1. Seleciona as colunas de junção ('queen') e a coluna a adicionar ('nome')\n",
    "    on='queen',                    # 2. Chave de junção\n",
    "    how='left'                     # 3. Tipo de junção: Left Join (preserva todas as linhas de df_table)\n",
    ")\n",
    "\n",
    "df_table.to_excel(\"AAAAAAAA2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38cdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5565c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de61aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d46497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (com OpenCV)",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
