{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94013008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee190b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_por_pontuacao= pd.read_excel('queen_por_pontuacao.xlsx', sheet_name=0)\n",
    "queen_por_posicao= pd.read_excel('queen_por_pontuacao.xlsx', sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d07f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"id\",\"queen\",\"ep\",\"bom\",\"ruim\",\"media\",\"colocacao\",\"idade\",\"tempfranquia\",\"vencedora\"]\n",
    "df = pd.DataFrame(queen_por_pontuacao, columns=cols)\n",
    "num_cols = [\"ep\",\"bom\",\"ruim\",\"media\",\"colocacao\",\"idade\",\"tempfranquia\",\"vencedora\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f4a4370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sequências (queen x temporada): 629\n",
      "Distribuição de labels: {0: 575, 1: 54}\n",
      "Exemplo meta[0]: {'season': 101, 'queen': 'Akashia', 'n_eps': 3}\n",
      "Exemplo seq[0]:\n",
      " [[ 1. 22.  0.  5. -2.]\n",
      " [ 2. 22.  0. 10. -4.]\n",
      " [ 3. 22.  0. 16. -7.]]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\"ep\",\"idade\", \"bom\",\"ruim\",\"media\"]  # features por episódio\n",
    "groups = df.groupby([\"tempfranquia\",\"queen\"])\n",
    "sequences = []\n",
    "labels = []\n",
    "meta = []\n",
    "for (season, queen), g in groups:\n",
    "    g_sorted = g.sort_values(\"ep\")\n",
    "    seq = g_sorted[feature_cols].values.astype(float)\n",
    "    sequences.append(seq)\n",
    "    labels.append(int(g_sorted[\"vencedora\"].max()))  # 1 se venceu a temporada, 0 caso contrário\n",
    "    meta.append({\"season\": int(season), \"queen\": queen, \"n_eps\": len(g_sorted)})\n",
    "\n",
    "print(\"Total de sequências (queen x temporada):\", len(sequences))\n",
    "print(\"Distribuição de labels:\", pd.Series(labels).value_counts().to_dict())\n",
    "print(\"Exemplo meta[0]:\", meta[0])\n",
    "print(\"Exemplo seq[0]:\\n\", sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa7858cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>queen</th>\n",
       "      <th>ep</th>\n",
       "      <th>bom</th>\n",
       "      <th>ruim</th>\n",
       "      <th>media</th>\n",
       "      <th>colocacao</th>\n",
       "      <th>idade</th>\n",
       "      <th>tempfranquia</th>\n",
       "      <th>vencedora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>BeBe Zahara Benet</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>629</td>\n",
       "      <td>Kara Might</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>629</td>\n",
       "      <td>Kara Might</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>629</td>\n",
       "      <td>Kara Might</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>630</td>\n",
       "      <td>Srirasha Hotsauce</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>630</td>\n",
       "      <td>Srirasha Hotsauce</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2677 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id              queen   ep   bom  ruim  media  colocacao  idade  \\\n",
       "0       1  BeBe Zahara Benet  1.0   1.0   1.0    0.0        1.0   28.0   \n",
       "1       1  BeBe Zahara Benet  2.0   2.0   2.0    0.0        1.0   28.0   \n",
       "2       1  BeBe Zahara Benet  3.0   8.0   2.0    2.0        1.0   28.0   \n",
       "3       1  BeBe Zahara Benet  4.0  12.0   2.0    3.0        1.0   28.0   \n",
       "4       1  BeBe Zahara Benet  5.0  12.0   7.0    1.0        1.0   28.0   \n",
       "...   ...                ...  ...   ...   ...    ...        ...    ...   \n",
       "4025  629         Kara Might  1.0   4.0   0.0    1.0        9.0   22.0   \n",
       "4026  629         Kara Might  2.0   5.0   1.0    1.0        9.0   22.0   \n",
       "4027  629         Kara Might  3.0   5.0   7.0   -2.0        9.0   22.0   \n",
       "4028  630  Srirasha Hotsauce  1.0   0.0   5.0   -2.0       10.0   27.0   \n",
       "4029  630  Srirasha Hotsauce  2.0   0.0  11.0   -5.0       10.0   27.0   \n",
       "\n",
       "      tempfranquia  vencedora  \n",
       "0            101.0        1.0  \n",
       "1            101.0        1.0  \n",
       "2            101.0        1.0  \n",
       "3            101.0        1.0  \n",
       "4            101.0        1.0  \n",
       "...            ...        ...  \n",
       "4025         307.0        0.0  \n",
       "4026         307.0        0.0  \n",
       "4027         307.0        0.0  \n",
       "4028         307.0        0.0  \n",
       "4029         307.0        0.0  \n",
       "\n",
       "[2677 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccd174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d905c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a63a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (padded): (629, 14, 4)  y shape: (629,)\n"
     ]
    }
   ],
   "source": [
    "all_steps = np.vstack(sequences)  # concat de todos os episódios (2D)\n",
    "scaler = StandardScaler().fit(all_steps)\n",
    "sequences_scaled = [scaler.transform(s) for s in sequences]\n",
    "\n",
    "# pad_sequences: tenta importar do tensorflow, se não existir usa implementação numpy\n",
    "try:\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    use_tf_pad = True\n",
    "except Exception:\n",
    "    use_tf_pad = False\n",
    "\n",
    "def pad_sequences_np(sequences, maxlen=None, dtype=\"float32\", padding=\"post\", value=0.0):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(s) for s in sequences)\n",
    "    n_features = sequences[0].shape[1]\n",
    "    X = np.full((len(sequences), maxlen, n_features), fill_value=value, dtype=dtype)\n",
    "    for i, s in enumerate(sequences):\n",
    "        length = min(len(s), maxlen)\n",
    "        if padding == \"post\":\n",
    "            X[i, :length, :] = s[:length, :]\n",
    "        else:\n",
    "            X[i, -length:, :] = s[:length, :]\n",
    "    return X\n",
    "\n",
    "max_len = max(len(s) for s in sequences_scaled)\n",
    "n_features = sequences_scaled[0].shape[1]\n",
    "if use_tf_pad:\n",
    "    X = pad_sequences(sequences_scaled, maxlen=max_len, dtype=\"float32\", padding=\"post\", value=0.0)\n",
    "else:\n",
    "    X = pad_sequences_np(sequences_scaled, maxlen=max_len, dtype=\"float32\", padding=\"post\", value=0.0)\n",
    "\n",
    "y = np.array(labels).astype(\"float32\")\n",
    "print(\"X shape (padded):\", X.shape, \" y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "127bf2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem labels: {0: 575, 1: 54}\n",
      "Shapes -> X_train: (503, 14, 4) X_test: (126, 14, 4)\n",
      "class_weight: {0: 0.5467391304347826, 1: 5.848837209302325}\n"
     ]
    }
   ],
   "source": [
    "# Bloco 5: train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Contagem labels:\", dict(zip(unique.astype(int), counts)))\n",
    "stratify_param = y if (len(np.unique(y))>1 and np.min(np.bincount(y.astype(int)))>1) else None\n",
    "\n",
    "X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n",
    "    X, y, meta, test_size=0.2, random_state=42, stratify=stratify_param\n",
    ")\n",
    "print(\"Shapes -> X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "# class weight (útil se houver muito desbalanceamento)\n",
    "if len(np.unique(y_train))>1:\n",
    "    classes = np.unique(y_train.astype(int))\n",
    "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train.astype(int))\n",
    "    class_weight = dict(zip(classes, cw))\n",
    "    print(\"class_weight:\", class_weight)\n",
    "else:\n",
    "    class_weight = None\n",
    "    print(\"Não há variação de classes no treino para calcular class_weight.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4fb416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo compilado. Resumo:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m4\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,281</span> (20.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,281\u001b[0m (20.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,281</span> (20.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,281\u001b[0m (20.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51/51 - 3s - 53ms/step - accuracy: 0.4701 - loss: 0.6252 - val_accuracy: 0.7228 - val_loss: 0.6522\n",
      "Epoch 2/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7214 - loss: 0.5078 - val_accuracy: 0.7822 - val_loss: 0.5484\n",
      "Epoch 3/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7562 - loss: 0.4441 - val_accuracy: 0.8317 - val_loss: 0.4300\n",
      "Epoch 4/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7687 - loss: 0.4001 - val_accuracy: 0.8317 - val_loss: 0.4248\n",
      "Epoch 5/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7687 - loss: 0.3802 - val_accuracy: 0.8317 - val_loss: 0.4128\n",
      "Epoch 6/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7662 - loss: 0.3677 - val_accuracy: 0.8416 - val_loss: 0.4047\n",
      "Epoch 7/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7562 - loss: 0.3629 - val_accuracy: 0.8317 - val_loss: 0.3939\n",
      "Epoch 8/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7687 - loss: 0.3422 - val_accuracy: 0.8317 - val_loss: 0.3822\n",
      "Epoch 9/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7711 - loss: 0.3436 - val_accuracy: 0.8317 - val_loss: 0.3925\n",
      "Epoch 10/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7711 - loss: 0.3378 - val_accuracy: 0.8317 - val_loss: 0.3957\n",
      "Epoch 11/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7662 - loss: 0.3362 - val_accuracy: 0.8317 - val_loss: 0.3972\n",
      "Epoch 12/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7662 - loss: 0.3358 - val_accuracy: 0.8317 - val_loss: 0.4082\n",
      "Epoch 13/20\n",
      "51/51 - 0s - 5ms/step - accuracy: 0.7587 - loss: 0.3375 - val_accuracy: 0.8317 - val_loss: 0.3928\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Treino finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Bloco 6: construir e treinar RNN — requer tensorflow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0., input_shape=(max_len, n_features)),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    print(\"Modelo compilado. Resumo:\")\n",
    "    model.summary()\n",
    "\n",
    "    # Treino (poucas épocas para demo — aumente quando tiver mais dados)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2 if len(X_train)>1 else 0.0,\n",
    "        epochs=20,\n",
    "        batch_size=8,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=[es],\n",
    "        verbose=2\n",
    "    )\n",
    "    print(\"Treino finalizado.\")\n",
    "except Exception as e:\n",
    "    print(\"Erro: não foi possível importar TensorFlow ou treinar. Erro:\", e)\n",
    "    print(\"Se você ainda não instalou TensorFlow, rode: !pip install tensorflow e execute este bloco novamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7340e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste - loss: 0.3019, accuracy: 0.8413\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "ROC AUC: 0.9256916996047431\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.83      0.90       115\n",
      "         1.0       0.35      1.00      0.52        11\n",
      "\n",
      "    accuracy                           0.84       126\n",
      "   macro avg       0.68      0.91      0.71       126\n",
      "weighted avg       0.94      0.84      0.87       126\n",
      "\n",
      "\n",
      "Exemplos de predições:\n",
      "Queen: Leona Winter, season: 3015, n_eps: 7, true: 0, prob: 0.8071\n",
      "Queen: Sanjina DaBish Queen, season: 504, n_eps: 4, true: 0, prob: 0.1457\n",
      "Queen: Alexis Mateo, season: 301, n_eps: 12, true: 0, prob: 0.3458\n",
      "Queen: Amanda Tears, season: 1012, n_eps: 2, true: 0, prob: 0.0403\n",
      "Queen: Jade Sotomayor, season: 101, n_eps: 4, true: 0, prob: 0.0347\n",
      "Queen: Sherry Pie, season: 1201, n_eps: 12, true: 0, prob: 0.7900\n",
      "Queen: Sminty Drop, season: 402, n_eps: 4, true: 0, prob: 0.1441\n",
      "Queen: Eva Blunt, season: 206, n_eps: 10, true: 1, prob: 0.8413\n",
      "\n",
      "Evolução de probabilidade com os primeiros k episódios (primeira sequência de teste):\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "k=1 -> prob: 0.3845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "k=2 -> prob: 0.4666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "k=3 -> prob: 0.5495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "k=4 -> prob: 0.6484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "k=5 -> prob: 0.7181\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "k=6 -> prob: 0.7819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "k=7 -> prob: 0.8071\n"
     ]
    }
   ],
   "source": [
    "# Bloco 7: avaliação e exemplos (requer que o modelo tenha sido treinado)\n",
    "try:\n",
    "    # avaliação\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Teste - loss: {:.4f}, accuracy: {:.4f}\".format(results[0], results[1]))\n",
    "\n",
    "    # predições (probabilidades)\n",
    "    y_prob = model.predict(X_test).ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    # métricas extras\n",
    "    from sklearn.metrics import roc_auc_score, classification_report\n",
    "    if len(np.unique(y_test))>1:\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    print(\"\\nClassification report (test):\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # mostrar exemplos\n",
    "    print(\"\\nExemplos de predições:\")\n",
    "    for i in range(min(8, len(y_test))):\n",
    "        print(f\"Queen: {meta_test[i]['queen']}, season: {meta_test[i]['season']}, n_eps: {meta_test[i]['n_eps']}, true: {int(y_test[i])}, prob: {y_prob[i]:.4f}\")\n",
    "\n",
    "    # demonstrar probabilidade usando apenas primeiros k episódios da primeira sequência de teste\n",
    "    def predict_with_first_k(seq_padded, k):\n",
    "        arr = seq_padded.copy()\n",
    "        if k < seq_padded.shape[0]:\n",
    "            arr[k:,:] = 0.0\n",
    "        return model.predict(arr[None, ...])[0,0]\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        idx = 0\n",
    "        print(\"\\nEvolução de probabilidade com os primeiros k episódios (primeira sequência de teste):\")\n",
    "        for k in range(1, meta_test[idx]['n_eps']+1):\n",
    "            print(f\"k={k} -> prob: {predict_with_first_k(X_test[idx], k):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Erro durante avaliação/predição — verifique se o modelo foi treinado. Erro:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb4bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Arquivo 'probabilidades_queens.xlsx' salvo!\n",
      "   season                 queen  n_eps  prob_vencedora  real_vencedora\n",
      "0    3015          Leona Winter      7        0.807085               0\n",
      "1     504  Sanjina DaBish Queen      4        0.145716               0\n",
      "2     301          Alexis Mateo     12        0.345755               0\n",
      "3    1012          Amanda Tears      2        0.040292               0\n",
      "4     101        Jade Sotomayor      4        0.034696               0\n",
      "5    1201            Sherry Pie     12        0.790034               0\n",
      "6     402           Sminty Drop      4        0.144131               0\n",
      "7     206             Eva Blunt     10        0.841298               1\n",
      "8     205         Beverly Kills      6        0.002638               0\n",
      "9     602       Zahirah Zapanta      4        0.012125               0\n"
     ]
    }
   ],
   "source": [
    "# 1) Obter as probabilidades\n",
    "y_probs = model.predict(X_test).flatten()\n",
    "\n",
    "# 2) Juntar meta_data das queens de teste\n",
    "results = []\n",
    "for idx, meta in enumerate(meta_test):   # meta_test = infos das queens no conjunto de teste\n",
    "    results.append({\n",
    "        \"season\": meta[\"season\"],\n",
    "        \"queen\": meta[\"queen\"],\n",
    "        \"n_eps\": meta[\"n_eps\"],\n",
    "        \"prob_vencedora\": float(y_probs[idx]),\n",
    "        \"real_vencedora\": int(y_test[idx])\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# 3) Exportar para Excel\n",
    "df_results.to_excel(\"probabilidades_queens.xlsx\", index=False)\n",
    "\n",
    "print(\"Arquivo 'probabilidades_queens.xlsx' salvo!\")\n",
    "print(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70237ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1,5) into shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m prefix = feats[:t]\n\u001b[32m     15\u001b[39m padded = np.zeros((max_len, n_features_model))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mpadded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m = prefix  \u001b[38;5;66;03m# garante compatibilidade\u001b[39;00m\n\u001b[32m     18\u001b[39m padded_batch = np.expand_dims(padded, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     19\u001b[39m prob = model.predict(padded_batch, verbose=\u001b[32m0\u001b[39m).ravel()[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: could not broadcast input array from shape (1,5) into shape (1,4)"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "n_features_model = X_train.shape[2]  # número de features que o modelo espera\n",
    "\n",
    "for (season, qid), g in queen_por_pontuacao.groupby([\"tempfranquia\",\"queen\"]):\n",
    "    g = g.sort_values(\"ep\")\n",
    "    nome = g[\"queen\"].iloc[0]\n",
    "\n",
    "    row = {\"queen\": nome, \"temporada\": season}\n",
    "    feats = g[[\"ep\",\"bom\",\"ruim\",\"media\",\"colocacao\"]].values\n",
    "\n",
    "    for t in range(1, len(feats)+1):\n",
    "        prefix = feats[:t]\n",
    "\n",
    "        padded = np.zeros((max_len, n_features_model))\n",
    "        padded[:t, :prefix.shape[1]] = prefix  # garante compatibilidade\n",
    "\n",
    "        padded_batch = np.expand_dims(padded, axis=0)\n",
    "        prob = model.predict(padded_batch, verbose=0).ravel()[0]\n",
    "\n",
    "        row[f\"episodio_{t}\"] = f\"{prob*100:.1f}%\"\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_excel(\"probabilidades_por_episodio.xlsx\", index=False)\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7140d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabb2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2102d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b8d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f103c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527c6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53213ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0ebe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ebeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d63feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e42b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85718a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (com OpenCV)",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
